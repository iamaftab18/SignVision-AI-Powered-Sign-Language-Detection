# ğŸš€ SignVision

## ğŸ“Œ Project Overview

**SignVision** is an AI-powered sign language recognition system that enables real-time sign detection and translation. It utilizes **MediaPipe** for landmark detection and **machine learning models** to classify gestures. The application is built using **Tkinter** for the UI and supports **SQLite** for user authentication and data storage.

---

## ğŸ›  Tech Stack

### Backend:
- **Python** (Tkinter-based GUI)
- **SQLite** (User authentication & data storage)
- **MediaPipe** (Real-time gesture detection)

### Frontend:
- **Tkinter** (User-friendly graphical interface)
- **OpenCV** (Camera integration & image processing)
- **Pyttsx3** (Text-to-speech support)

### Other Tools:
- **Scikit-learn** (Machine learning for sign recognition)
- **NumPy & Joblib** (Data processing & model storage)

---

## ğŸ“Œ Features
- ğŸ” **User Authentication** (Register/Login using SQLite database)
- ğŸ¥ **Real-time Sign Detection** (MediaPipe & OpenCV integration)
- ğŸ§  **Machine Learning Model** (Trains and detects sign actions)
- ğŸ“ **Text Output** (Displays recognized actions as text)
- ğŸ”Š **Text-to-Speech Support** (Reads detected actions aloud)
- ğŸ¨ **Interactive UI** (Built using Tkinter)

---

## âš¡ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```sh
git clone https://github.com/iamaftab18/SignVision-AI-Powered-Sign-Language-Detection.git
cd SignVision-AI-Powered-Sign-Language-Detection
```

### 2ï¸âƒ£ Set Up Virtual Environment (Optional but Recommended)
```sh
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies
```sh
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the Application
```sh
python app.py
```

---

## ğŸ— Project Structure
```
SignVision-AI-Powered-Sign-Language-Detection/
â”‚â”€â”€ models/              # Trained ML models
â”‚â”€â”€ data/                # Stored sign gesture data
â”‚â”€â”€ app.py               # Main application file
â”‚â”€â”€ sign_language.db      # SQLite database
â”‚â”€â”€ requirements.txt      # Project dependencies
â”‚â”€â”€ README.md            # Project documentation
```

---

## ğŸ¯ How It Works
1ï¸âƒ£ **User registers/logs in** using the GUI.
2ï¸âƒ£ **Camera captures hand and face movements** using MediaPipe.
3ï¸âƒ£ **Extracted keypoints** are fed into the trained machine learning model.
4ï¸âƒ£ **Recognized gestures** are displayed as text.
5ï¸âƒ£ **Optional: Text-to-Speech reads out the detected actions.**

---


## ğŸ¤ Contributing
We welcome contributions! If you'd like to improve this project, follow these steps:
1. **Fork the repository** ğŸ´
2. **Create a new branch** ğŸ”€
3. **Make your changes** âœ¨
4. **Submit a pull request** ğŸ“©

---

## ğŸ“œ License
This project is licensed under the **MIT License**.

---

## ğŸ“ Contact
ğŸ‘¨â€ğŸ’» **Your Name**  
ğŸ“§ Email: aftabsm18@gmail.com 
ğŸ”— GitHub: [iamaftab18](https://github.com/iamaftab18)  

---

_ğŸ’™ Thank you for checking out SignVision! Let's make sign language more accessible with AI!_ ğŸš€

